{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: [Dies Natalis Solis Invicti](https://en.wikipedia.org/wiki/Sol_Invictus)\n",
    "* Reviews: [Befana](https://en.wikipedia.org/wiki/Befana)\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import namedtuple, defaultdict\n",
    "import random as random\n",
    "from copy import deepcopy\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "State = namedtuple('State', ['x', 'o'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAGIC = [2, 7, 6, 9, 5, 1, 4, 3, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_board(pos):\n",
    "    \"\"\"Nicely prints the board\"\"\"\n",
    "    for r in range(3):\n",
    "        for c in range(3):\n",
    "            i = r * 3 + c\n",
    "            if MAGIC[i] in pos.x:\n",
    "                print('X', end='')\n",
    "            elif MAGIC[i] in pos.o:\n",
    "                print('O', end='')\n",
    "            else:\n",
    "                print('.', end='')\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "def win(elements):\n",
    "    \"\"\"Checks is elements is winning\"\"\"\n",
    "    return any(sum(c) == 15 for c in combinations(elements, 3))\n",
    "\n",
    "def state_value(pos: State):\n",
    "    \"\"\"Evaluate state: +1 first player wins\"\"\"\n",
    "    if win(pos.x):\n",
    "        return 1\n",
    "    elif win(pos.o):\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_game():\n",
    "    trajectory = list()\n",
    "    state = State(set(), set())\n",
    "    available = set(range(1, 9+1))\n",
    "    while available:\n",
    "        x = random.choice(list(available))\n",
    "        state.x.add(x)\n",
    "        trajectory.append(deepcopy(state))\n",
    "        available.remove(x)\n",
    "        if win(state.x) or not available:\n",
    "            break\n",
    "\n",
    "        o = random.choice(list(available))\n",
    "        state.o.add(o)\n",
    "        trajectory.append(deepcopy(state))\n",
    "        available.remove(o)\n",
    "        if win(state.o):\n",
    "            break\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvalue_dictionary = defaultdict(float)\\nhit_state = defaultdict(int)\\nepsilon = 0.001\\n\\nfor steps in tqdm(range(500_000)):\\n    trajectory = random_game()\\n    final_reward = state_value(trajectory[-1])\\n    for state in trajectory:\\n        hashable_state = (frozenset(state.x), frozenset(state.o))\\n        hit_state[hashable_state] += 1\\n        value_dictionary[hashable_state] = value_dictionary[\\n            hashable_state\\n        ] + epsilon * (final_reward - value_dictionary[hashable_state])\\n'"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "value_dictionary = defaultdict(float)\n",
    "hit_state = defaultdict(int)\n",
    "epsilon = 0.001\n",
    "\n",
    "for steps in tqdm(range(500_000)):\n",
    "    trajectory = random_game()\n",
    "    final_reward = state_value(trajectory[-1])\n",
    "    for state in trajectory:\n",
    "        hashable_state = (frozenset(state.x), frozenset(state.o))\n",
    "        hit_state[hashable_state] += 1\n",
    "        value_dictionary[hashable_state] = value_dictionary[\n",
    "            hashable_state\n",
    "        ] + epsilon * (final_reward - value_dictionary[hashable_state])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(value_dictionary.items(), key=lambda e: e[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "#size = dim playing grid\n",
    "#learning_rate = Check how much the agent should update its values \n",
    "#discount_factor = This value reflects how much the agent takes into account future rewards during learning\n",
    "#exploration_prob = Represents the probability that the agent performs random action instead of choosing the action that maximizes the value Q.\n",
    "class QAgent:\n",
    "    def __init__(self, size=3, learning_rate=0.1, discount_factor=0.9, exploration_prob=0.1):\n",
    "        self.size = size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_prob = exploration_prob\n",
    "\n",
    "    \n",
    "        state = State(tuple(), tuple()) \n",
    "        action = 0\n",
    "        self.Q = {(state, action): 0 }\n",
    "\n",
    "    def find_state(self, my_dictionary, state_to_find):\n",
    "        for this_tuple, valore in my_dictionary.items():\n",
    "            if this_tuple[0] == state_to_find:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def find_max(self, my_dictionary, state_to_find):\n",
    "        tuple_filtrate = {k: v for k, v in my_dictionary.items() if k[0] == state_to_find}\n",
    "\n",
    "        if not tuple_filtrate:\n",
    "            # Handle the case when tuple_filtrate is empty\n",
    "            return -1\n",
    "\n",
    "        tupla_massima = max(tuple_filtrate, key=lambda k: my_dictionary[k])\n",
    "        return tupla_massima[1]\n",
    "\n",
    "\n",
    "    def choose_action(self, state, available):\n",
    "        if random.uniform(0, 1) < self.exploration_prob:\n",
    "            return random.choice(list(available))\n",
    "        else:\n",
    "            #best action to do in that state\n",
    "            if ((state != State(tuple(), tuple()) ) and self.find_state(self.Q, state) ):\n",
    "                return self.find_max(self.Q, state)\n",
    "            else:\n",
    "                return random.choice(list(available))\n",
    "\n",
    "    def update_q_value(self, state, action, next_state, reward, available):\n",
    "        #current_q = self.Q[state, action]\n",
    "        current_q = self.Q.get((state, action), 0)  # Restituisce 0 se la chiave non esiste\n",
    "        #best_next_q = max([self.Q[next_state, next_action] for next_action in available], default=0)\n",
    "        best_next_q = max([self.Q.get((next_state, next_action), 0) for next_action in available], default=0)\n",
    "        self.Q[state, action] = (1 - self.learning_rate) * current_q + self.learning_rate * (reward + self.discount_factor * best_next_q)\n",
    "\n",
    "    def take_action(self, state: State, action, player, available):\n",
    "        # Check if the cell is already taken\n",
    "        if action in state.x or action in state.o:\n",
    "            # if the cell is taken, retun current state, a negative reward and that the game is not over\n",
    "            return state, -1, False\n",
    "        \n",
    "        #if i am here the action is valid so i can remove it from available \n",
    "        available.remove(action)\n",
    "\n",
    "        # Copies the current state so you don't change it directly\n",
    "        next_state = deepcopy(state)\n",
    "\n",
    "        # play the action in x or o according to the player turn\n",
    "        if(player[0] == \"x\"):\n",
    "            new_x = tuple(sorted(next_state.x + (action,)))\n",
    "            next_state = State(new_x, state.o)\n",
    "            player[0] = \"o\"\n",
    "        else:\n",
    "            new_o = tuple(sorted(next_state.o + (action,)))\n",
    "            next_state = State(state.x, new_o)\n",
    "            player[0] = \"x\"\n",
    "\n",
    "        # Check if there is a win or if the game ended in a draw\n",
    "        if win(next_state.x) or win(next_state.o) :\n",
    "            done = True\n",
    "            reward = 1  # positive reward for win the game\n",
    "        elif not available:\n",
    "            done = True\n",
    "            reward = 0  # Zero reward for draw\n",
    "        else:\n",
    "            done = False\n",
    "            reward = 0  # No reward if the game is not over\n",
    "\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def play(self, episodes=1000):\n",
    "        for _ in tqdm(range(episodes)):\n",
    "            #initial state\n",
    "            state = State(tuple(), tuple())\n",
    "            #avaiable moves for this episode\n",
    "            available = set(range(1, 9+1))\n",
    "            #random choose player order\n",
    "            player = [random.choice((\"x\", \"o\"))]\n",
    "            #check if the game is over\n",
    "            done = False\n",
    "\n",
    "            #Play until there are moves to make and neither player has won\n",
    "            while not done:\n",
    "                # control whether to take a move from those available or from Q table\n",
    "                action = self.choose_action(state, available)\n",
    "                # calculate next_state, raward and in the game is over\n",
    "                next_state, reward, done = self.take_action(state, action, player, available)\n",
    "                self.update_q_value(state, action, next_state, reward, available)\n",
    "                state = next_state\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1865.67it/s]\n"
     ]
    }
   ],
   "source": [
    "q_agent = QAgent()\n",
    "q_agent.play(episodes=100_000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
